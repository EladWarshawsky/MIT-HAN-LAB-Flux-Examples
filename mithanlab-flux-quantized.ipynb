{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8ae24f9-fa18-47e1-8b3a-bd357ee1f1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bac0183-7cae-45b3-afdc-d2cbcbee1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-21 02:44:26.274] [info] Initializing QuantizedFluxModel\n",
      "[2025-01-21 02:44:26.281] [info] Loading weights from /home/ubuntu/.cache/huggingface/hub/models--mit-han-lab--svdq-int4-flux.1-dev/snapshots/1357abfc3979e1036f1e246f2398f814de245d20/transformer_blocks.safetensors\n",
      "[2025-01-21 02:44:27.573] [info] Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expected types for transformer: ['FluxTransformer2DModel'], got NunchakuFluxTransformer2dModel.\n",
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]\n",
      "\u001b[Ading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[Ading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  6.88it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  7.12it/s]\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  8.12it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import FluxPipeline\n",
    "from nunchaku.models.transformer_flux import NunchakuFluxTransformer2dModel\n",
    "import time\n",
    "\n",
    "# Load the model and pipeline\n",
    "transformer = NunchakuFluxTransformer2dModel.from_pretrained(\"mit-han-lab/svdq-int4-flux.1-dev\")\n",
    "pipeline = FluxPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-dev\", transformer=transformer, torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59974e54-154b-4682-82f1-3347f8315565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting para-attn\n",
      "  Downloading para_attn-0.3.10.tar.gz (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "  Installing build dependencies ... \u001b[done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from para-attn) (2.5.1)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from torch->para-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from sympy==1.13.1->torch->para-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/envs/nunchaku/lib/python3.11/site-packages (from jinja2->torch->para-attn) (3.0.2)\n",
      "Building wheels for collected packages: para-attn\n",
      "  Building wheel for para-attn (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for para-attn: filename=para_attn-0.3.10-py2.py3-none-any.whl size=31162 sha256=ad5d5830fb0b27dd2bcde7ca61c4c1d92639cc70c0a7e4638e33c7cbdc8c74de\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/69/59/83/6faaa9fb85f80626b5cf622317180b76769b99d7942c682ef5\n",
      "Successfully built para-attn\n",
      "Installing collected packages: para-attn\n",
      "Successfully installed para-attn-0.3.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install para-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc8539fe-accc-4a9a-8816-42178bba2168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available port: 43595\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind((\"\", 0))  # Bind to an available port\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "free_port = find_free_port()\n",
    "print(f\"Available port: {free_port}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a08bac95-6367-466b-99a3-589f88ffd461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process group initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Set required environment variables\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"  # Replace with the master node's IP if distributed\n",
    "os.environ[\"MASTER_PORT\"] = \"43595\"  # Replace with an available port\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"  # Total number of processes (1 for local testing)\n",
    "os.environ[\"RANK\"] = \"0\"  # Rank of the current process (0 for the master process)\n",
    "\n",
    "# Initialize the process group\n",
    "dist.init_process_group(\n",
    "    backend=\"nccl\",  # Use \"gloo\" for CPU\n",
    "    init_method=\"env://\",\n",
    "    world_size=1,\n",
    "    rank=0,\n",
    ")\n",
    "\n",
    "print(\"Process group initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62a7e98f-8170-49b5-9c93-4a5111af8c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(16, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "# dist.init_process_group()\n",
    "\n",
    "torch.cuda.set_device(dist.get_rank())\n",
    "\n",
    "# vae = AutoencoderKL.from_pretrained(\n",
    "#     \"black-forest-labs/FLUX.1-dev\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "from para_attn.parallel_vae.diffusers_adapters import parallelize_vae\n",
    "\n",
    "parallelize_vae(pipeline.vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2e62bce-8c46-494a-bf68-c9fd6614df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:18<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image generation took 19.09 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure execution time\n",
    "start_time = time.time()\n",
    "image = pipeline(\"A cat holding a sign that says hello world\", num_inference_steps=50, guidance_scale=0).images[0]\n",
    "end_time = time.time()\n",
    "\n",
    "# Save the image\n",
    "image.save(\"example.png\")\n",
    "\n",
    "# Print runtime\n",
    "print(f\"Image generation took {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0430d298-eaa5-40c4-b882-c7b8b3d00dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated memory: 19237.60 MB\n",
      "Cached memory: 22992.00 MB\n",
      "Total GPU memory: 40326.38 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Total memory currently allocated on the GPU by PyTorch (in bytes)\n",
    "allocated_memory = torch.cuda.memory_allocated() / (1024 ** 2)  # Convert to MB\n",
    "print(f\"Allocated memory: {allocated_memory:.2f} MB\")\n",
    "\n",
    "# Total memory currently cached by PyTorch (in bytes)\n",
    "cached_memory = torch.cuda.memory_reserved() / (1024 ** 2)  # Convert to MB\n",
    "print(f\"Cached memory: {cached_memory:.2f} MB\")\n",
    "\n",
    "# Total memory capacity of the GPU (in bytes)\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)  # Convert to MB\n",
    "print(f\"Total GPU memory: {total_memory:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nunchaku)",
   "language": "python",
   "name": "nunchaku"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
